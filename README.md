# ğŸ˜ï¸ crawler-real-estate-template

Template estruturado para scraping de imÃ³veis com foco em extraÃ§Ã£o limpa, anizaÃ§Ã£o modular e pronto para uso em Data Science, integraÃ§Ã£o com LLMs e ortaÃ§Ã£o em CSV/Parquet.

---

## ğŸš€ Objetivo

Este repositÃ³rio serve como **modelo base** para projetos de scraping do cado imobiliÃ¡rio. A estrutura foi pensada para:

- Separar a coleta (`scrapy` + `playwright`) da transformaÃ§Ã£o dos dados.
- Exportar dados diretamente para anÃ¡lises.
- Enriquecer os resultados com tÃ©cnicas de **fuzzy matching** e **modelos **.

---

## âš™ï¸ Requisitos

### ğŸ“¦ DependÃªncias (requirements.txt)

```txt
# WebScraping
scrapy
playwright
scrapy-playwright

# Data Science + Utils
pandas
rapidfuzz
requests
streamlit
pyarrow

# LLM
llama-index
llama-index-llms-ollama
langchain
langchain-community
```

---

## ğŸ“¥ InstalaÃ§Ã£o

> Recomendado: ambiente virtual (venv ou conda)

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/crawler-real-estate-template.git
cd crawler-real-estate-template

# Instale as dependÃªncias
pip install -r requirements.txt

# Instale o Playwright
playwright install
```

---

## â–¶ï¸ ExecuÃ§Ã£o

O pipeline completo Ã© iniciado via `main.py`, que:

1. Executa o spider Scrapy com Playwright.
2. Processa e organiza os dados coletados.
3. Salva arquivos CSV e Parquet em pastas nomeadas por data/hora.

```bash
python main.py
```

Exemplo de saÃ­da no terminal:

```
Iniciando o pipeline de scraping para EvidenceImoveis - 2025_07_28 14_03_10
Pipeline finalizado!
```

---

## ğŸ“‚ Onde os dados sÃ£o salvos?

Ao rodar o `main.py`, a seguinte estrutura serÃ¡ criada automaticamente:

```
/data/YYYY_MM_DD/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ HH_MM_SS/
â”‚       â””â”€â”€ EvidenceImoveis.json
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ imovel.csv
â”‚   â”œâ”€â”€ endereco.csv
â”‚   â”œâ”€â”€ caracteristicas.csv
â”‚   â”œâ”€â”€ imovel.parquet
â”‚   â””â”€â”€ ...
```

![Exemplo de ExtraÃ§Ã£o](https://raw.githubusercontent.com/PardoMarques/crawler-real-estate-template/refs/heads/main/extracao.png)

---

## ğŸ§  IntegraÃ§Ã£o com LLM

O pipeline `LLMCleanRealStatePipeline` detecta nomes de condomÃ­nios em criÃ§Ãµes usando LLMs locais (via Ollama).

âš™ï¸ Verifique o funcionamento do Ollama e o modelo LLM instalado no script:

```python
from llm.ollama_client import extrair_condominio_ollama
```

---

## ğŸ“Œ Exemplo de Spider

A pasta `spiders/` contÃ©m exemplos reais, como o `EvidenceImoveis.py` ou o `VisaoGlobalImoveis.py`, que cadastram imÃ³veis em SÃ£o Paulo com critÃ©rios definidos.  
A estrutura pode ser adaptada para qualquer outro portal, MAS seguindo majoritariamente a classe definida em Items, para que toda integraÃ§Ã£o de anÃ¡lise dos dados possam futuramente obter sucesso.

---

## CONTRIBUIÃ‡ÃƒO:

## Pull Requests serÃ£o bem-vindos!

## MAS neste momento ainda estou organizando a base principal

---

## ğŸ§­ Roadmap Futuro (prÃ³ximas versÃµes)

- [ ] Dashboard com Streamlit
- [ ] Arquitetura LLM -> Boas PrÃ¡ticas + Prompt Engineering
- [ ] Disponibilizar curso gratuÃ­to para utilizaÃ§Ã£o do template

---

## ğŸ“„ LicenÃ§a

MIT License.

---

## ğŸ‘¨â€ğŸ’» Autor

Projeto iniciado por [Caio Marques](https://github.com/pardomarques), com o intuito de promover scraping Ã©tico e padronizado no mercado imobiliÃ¡rio brasileiro.
